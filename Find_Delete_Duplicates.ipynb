{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Find and Delete Duplicates using imagededup,resnet50, and tensorflow provided with the help of ChatGPT"
      ],
      "metadata": {
        "id": "AF_dohOkun8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install ultralytics  # install\n",
        "from ultralytics import YOLO, checks, hub\n",
        "checks()  # checks"
      ],
      "metadata": {
        "id": "bZZ_A-ybuOti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Dependencies"
      ],
      "metadata": {
        "id": "71bafFX8ukJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tensorflow keras imageHash torch torchvision imagededup"
      ],
      "metadata": {
        "id": "QB23Y1eQuTRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Code to run"
      ],
      "metadata": {
        "id": "Nyin5podu553"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from keras.applications import ResNet50\n",
        "\n",
        "# Set GPU options\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "\n",
        "# Load the pre-trained ResNet50 model\n",
        "model = ResNet50(weights='imagenet')\n",
        "\n",
        "# Set the paths to the image and label directories\n",
        "train_image_dir = 'train/images'\n",
        "train_label_dir = 'train/labels'\n",
        "test_image_dir = 'test/images'\n",
        "test_label_dir = 'test/labels'\n",
        "valid_image_dir = 'valid/images'\n",
        "valid_label_dir = 'valid/labels'\n",
        "\n",
        "# Define a function to load an image\n",
        "def load_image(image_path):\n",
        "    # Load the image\n",
        "    image = tf.keras.preprocessing.image.load_img(image_path, target_size=(416, 416))\n",
        "    \n",
        "    # Convert the image to a NumPy array\n",
        "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "    \n",
        "    # Rescale the pixel values to be between -1 and 1\n",
        "    image = tf.keras.applications.resnet50.preprocess_input(image)\n",
        "    \n",
        "    # Return the image\n",
        "    return image\n",
        "\n",
        "# Define a function to calculate the similarity between two images\n",
        "def calculate_similarity(image_path_1, image_path_2):\n",
        "    # Load the images\n",
        "    image_1 = load_image(image_path_1)\n",
        "    image_2 = load_image(image_path_2)\n",
        "\n",
        "    # Resize the images to 224x224\n",
        "    #image_1 = image_1.resize((224, 224))\n",
        "    image_1 = np.array(Image.fromarray(image_1.astype(np.uint8)).resize((224, 224))).astype('float32')\n",
        "    #image_2 = image_2.resize((224, 224))\n",
        "    image_2 = np.array(Image.fromarray(image_2.astype(np.uint8)).resize((224, 224))).astype('float32')\n",
        "\n",
        "    # Convert the images to arrays\n",
        "    image_1 = tf.keras.preprocessing.image.img_to_array(image_1)\n",
        "    image_2 = tf.keras.preprocessing.image.img_to_array(image_2)\n",
        "\n",
        "    # Preprocess the images\n",
        "    image_1 = tf.keras.applications.resnet50.preprocess_input(image_1)\n",
        "    image_2 = tf.keras.applications.resnet50.preprocess_input(image_2)\n",
        "\n",
        "    # Get the image features using the ResNet50 model\n",
        "    feature_1 = model.predict(image_1[np.newaxis, ...])\n",
        "    feature_2 = model.predict(image_2[np.newaxis, ...])\n",
        "\n",
        "    # Calculate the cosine similarity between the image features\n",
        "    similarity = tf.keras.losses.cosine_similarity(feature_1, feature_2)\n",
        "    return similarity.numpy()[0]\n",
        "\n",
        "## Define a function to delete duplicates\n",
        "def delete_duplicates(image_dir, label_dir):\n",
        "    # Get the list of image files in the directory\n",
        "    image_files = sorted(os.listdir(image_dir))\n",
        "    \n",
        "    # Initialize a list to store the indices of duplicate images\n",
        "    duplicate_indices = []\n",
        "    \n",
        "    # Loop over all pairs of images\n",
        "    for i in tqdm(range(len(image_files))):\n",
        "        for j in range(i+1, len(image_files)):\n",
        "            # Check if the images are similar\n",
        "            similarity = calculate_similarity(os.path.join(image_dir, image_files[i]), os.path.join(image_dir, image_files[j]))\n",
        "            if similarity > 0.95:\n",
        "                # If the images are similar, add the index of the second image to the list of duplicate indices\n",
        "                duplicate_indices.append(j)\n",
        "\n",
        "    # Delete the duplicate images and labels\n",
        "    for index in tqdm(sorted(duplicate_indices, reverse=True)):\n",
        "        os.remove(os.path.join(image_dir, image_files[index]))\n",
        "        os.remove(os.path.join(label_dir, image_files[index][:-4] + '.txt'))\n",
        "\n",
        "# Delete duplicates in the train directory\n",
        "print(\"Deleting duplicates in the train directory\")\n",
        "delete_duplicates(train_image_dir, train_label_dir)\n",
        "\n",
        "# Delete duplicates in the test directory\n",
        "print(\"Deleting duplicates in the test directory\")\n",
        "delete_duplicates(test_image_dir, test_label_dir)\n",
        "\n",
        "# Delete duplicates in the valid directory\n",
        "print(\"Deleting duplicates in the valid directory\")\n",
        "delete_duplicates(valid_image_dir, valid_label_dir)\n"
      ],
      "metadata": {
        "id": "cLuIS4kB4_Ga"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}